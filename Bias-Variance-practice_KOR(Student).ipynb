{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Analysis \n",
    "* 본 실습 내용\n",
    "1. Polynomial Regression 모델 만들기\n",
    "2. Bias and Variance Trade off 관계 이해\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Import\n",
    "* 아래 셀은 필요한 기능을 할 수 있는 함수들 (Pacakge)를 불러오는 과정입니다\n",
    "---\n",
    "패키지 설명\n",
    "* random : 난수 생성 \n",
    "* numpy : 행렬 연산\n",
    "* matplotlib.pyplot : 그래프 그리기\n",
    "* collections : 자료형\n",
    "* sklearn : 머신러닝 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Packages\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression (다항 회귀)\n",
    "* Linear Regerssion (선형회귀)의 연장선인 Polynomial Regression (다항 회귀) 모델 생성\n",
    "* Polynomial Regression 모델로 (degree=2) 원래 함수 (True distribution) 추정\n",
    "---\n",
    "기존에 숨겨져 있는, 알려지지 않는 함수가 있을 떄 Polynomial regression을 통해 얼마나 근사시킬 수 있을지 확인해봅시다. <br>\n",
    "다음과 같은 순서로 진행됩니다. <br>\n",
    "1. 예측할 함수 생성\n",
    "2. Polynomial Transform\n",
    "3. Polynomial regression model 만들기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측할 함수 생성 (degree=3)\n",
    "* x_ex와 y_ex 변수를 통해 함수 생성\n",
    "* np.arange(p) 는 주어진 p 값까지로 이루어진 numpy 행렬을 만들라는 것\n",
    "* plt.plot(x,y) : x 와 y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making polynomial regression model\n",
    "X_ex = np.arange(5) \n",
    "Y_ex =  3 - 2 * X_ex + X_ex ** 2 - X_ex ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f25350406a0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_ex,Y_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice 1. 다항 함수 만들기\n",
    "1. 정의역 x_temp : 범위가 [0,1,2,3,4,5,6,7,8] 까지의 행렬을 만들기\n",
    "2. 치역 y_temp = 10 * X_temp**3 + 7 * X_temp -10 만들기\n",
    "3. Ploting 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-b946037042b6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-b946037042b6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    X_temp = np.arange(??)\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "X_temp = np.arange(??)\n",
    "Y_temp = ??*X_temp**?? + ??*X_temp - ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_temp,Y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Transform : 일차항을  차수 d 차 항들로 변환\n",
    "* PolynomialFeatures(degree=d) : input 을 차수가 d 인 다항 features로 바꿔준다\n",
    "* poly.fit_transform(X[:, np.newaxis]) : poly 라는 instnace에, X_ex 데이터를 input으로 넣는다. np.newaxis (차원을 추가)\n",
    "* ex ) degree =3 , input : x -> output : c(상수), x , x^2 , x^3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "X_ex_trans = poly.fit_transform(X_ex[:, np.newaxis])\n",
    "\n",
    "print(\"Input\")\n",
    "print(X_ex)\n",
    "print(\"Output of Polynomial Features\")\n",
    "print(X_ex_trans)\n",
    "for row in range(X_ex_trans.shape[0]):\n",
    "    print(\"constant : {}, x1 : {}, x1**2 : {}\".format(X_ex_trans[row,0],X_ex_trans[row,1], X_ex_trans[row,2] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_ex shape : {} , X_ex[:,np.newaxis] shape : {}\".format(X_ex.shape, X_ex[:,np.newaxis].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice 2. Polynomial Features 만들기\n",
    "1. degree = 3 인 Polynomial Feature 를 poly_practice로 선언해보세요\n",
    "2. input X_ex를 1에서 선언한 poly_practice를 이용하여 변환해 보세요\n",
    "3. 변환 결과를 출력해보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_practice = PolynomialFeatures(degree=?)\n",
    "X_ex_trans = poly_practice.fit_transform(X_ex[:, ??])\n",
    "\n",
    "print(\"Input\")\n",
    "print(X_ex)\n",
    "print(\"Output of Polynomial Features\")\n",
    "print(X_ex_trans)\n",
    "for row in range(??):\n",
    "    print(\"constant : {}, x1 : {}, x1**2 : {}, x1**3 : {}\".format(X_ex_trans[row,0],X_ex_trans[row,1], X_ex_trans[row,2], X_ex_trans[row,3] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression : Polynomial Transfrom + Linear regression\n",
    "* Polynomial regression을 통해 input x 와 y를 학습\n",
    "---\n",
    "* Pipeline : sklearn에서 제공하는 함수들을 엮어서 하나의 함수로 변환\n",
    "* model.fit(X,Y) : input X와 Y 를 통해 model을 학습한다\n",
    "* model.named_steps['linear'].coef_ : linear 이름의 함수의 계수 (coefficent)를 가져온다\n",
    "* plt.scatter(x,y) : x, y 좌표를 찍은 산포도를(Scatter plot) 생성한다\n",
    "* plt.lengend() : 범례 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('poly', PolynomialFeatures(degree=2)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])\n",
    "model.fit(X_ex[:, np.newaxis], Y_ex)\n",
    "coefs = model.named_steps['linear'].coef_\n",
    "print(\"Final Equation : y = {:.2f} + {:.2f}*x1 + {:.2f}*(x1**2)\".format(\n",
    "    coefs[0],coefs[1],coefs[2]\n",
    "))\n",
    "print(\"Original Equation y =  3 - 2 * x1 + (x1 ** 2) - (x1**3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ex_pred = model.predict(X_ex[:, np.newaxis])\n",
    "plt.scatter(X_ex,Y_ex_pred,label='Predicted')\n",
    "plt.plot(X_ex,Y_ex,label='Ground Truth',color='orange')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice 3. Polynomial Regression 만들기 \n",
    "1. degree = 3 인 Polynomial Regression 를 model_practice로 선언해보세요\n",
    "2. X_ex, Y_ex input으로 model_practice을 학습해보세요\n",
    "3. model_practice 의 계수를 출력해보세요 (상수포함 4개)\n",
    "4. Ground Truth와 Predicted 값을 그래프를 그려보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_practice = Pipeline([('poly', PolynomialFeatures(degree=??)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])\n",
    "model_practice.fit(??, ??)\n",
    "coefs = ??\n",
    "print(\"Final Equation : y = {:.2f} + {:.2f}*x1 + {:.2f}*(x1**2) + {:.2f}*(x1**3)\".format(\n",
    "    coefs[0],coefs[1],coefs[2], coefs[3]\n",
    "))\n",
    "print(\"Original Equation y =  3 - 2 * x1 + (x1 ** 2) - (x1**3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression 을 이용하여 Sine graph 예측하기\n",
    "* Polynomial 모델을 이용하여 다항 함수가 아닌 sine 함수를 근사해보도록 합시다\n",
    "1. 먼저 sine 함수 식을 통해서 Synthetic 데이터를 생성합니다\n",
    "2. Synthetic 데이터에 noise를 일부 추가하고, Train/Test set으로 나눕니다. \n",
    "3. Train set을 이용하여 모델을 학습하고 Test set을 이용하여 모델의 성능을 평가합니다\n",
    "4. Regression 문제의 성능 지표인 Mean Squared loss와 Bias , Variance를 측정해봅니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "random.seed(10)\n",
    "\n",
    "def get_y_true(x): # 실제 sin graph\n",
    "    y = np.sin(x)\n",
    "    return y\n",
    "\n",
    "def get_y_noise(x): # Sin + Random Noise\n",
    "    y = get_y_true(x) + np.random.uniform(-0.4, 0.4, len(x))\n",
    "    return y\n",
    "\n",
    "def get_sample(sample_ratio, x, y): # x, y 에서 sample ratio 만큼 추출\n",
    "    m = len(x)\n",
    "    m_new = int(np.round(sample_ratio*m))\n",
    "    ind = random.sample(range(m), m_new)\n",
    "    ind = np.sort(ind)\n",
    "    x_sample = x[ind]\n",
    "    y_sample = y[ind]\n",
    "    y_true_sample = get_y_true(x_sample)\n",
    "    return x_sample, y_sample, y_true_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sine Graph Data 만들기\n",
    "* np.linspace(a,b,c) : 범위 a ~ b 만큼 구간을 c 개 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "x_num = 200\n",
    "x_space = np.linspace(0,np.pi*4,x_num) # divide the space 0 to 10 in 100 pieces!\n",
    "y = get_y_true(x_space) # sine function\n",
    "plt.scatter(x_space,y) # Draw scatter plot\n",
    "plt.title(\"True Distribution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Train, Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "rand_indicies = list(range(0,x_num))\n",
    "random.shuffle(rand_indicies)\n",
    "rand_idicies_train, rand_idicies_test =rand_indicies[:int(x_num*0.8)], rand_indicies[int(x_num*0.8):]\n",
    "x_train = x_space[rand_idicies_train]\n",
    "y_train = get_y_noise(x_train)\n",
    "\n",
    "x_test = x_space[rand_idicies_test]\n",
    "y_test = get_y_noise(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train,y_train) # Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_test,y_test)\n",
    "plt.title(\"Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression (degree 1) 로 Sine graph 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('poly', PolynomialFeatures(degree=1)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])\n",
    "model.fit(x_train[:, np.newaxis], y_train)\n",
    "y_predict = model.predict(x_train[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train,y_predict, label='Predicted', color ='black')\n",
    "plt.plot(x_space,y, label = 'Ground Truth', color='orange')\n",
    "plt.legend()\n",
    "plt.title(\"Ground Truth VS Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for containing the results\n",
    "result_dict = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Square Error 게산하기\n",
    "* mean_square_error(y1, y2) : y1 와 y2 mean squared error를 계산한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = model.predict(x_test[:,np.newaxis])\n",
    "## check Train, Test mse\n",
    "mse_train = mean_squared_error(y_train,y_predict)\n",
    "mse_test = mean_squared_error(y_test,y_test_predict)\n",
    "result_dict[1]['mse_train'] = mse_train\n",
    "result_dict[1]['mse_test'] = mse_test\n",
    "print(\"mse Train : {}, Test : {}\".format(mse_train,mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias 와 Variance 계산하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train data 에서 train sample 뽑기\n",
    "2. Train sample 를 이용하여 Polynomial regression model 학습\n",
    "3. Sample이 아닌 전체 train data 를 이용하여 모델의 예측값 계산\n",
    "4. 1,3 을 1000번 반복하면서 모델의 예측값 3 의 평균 계산 (exp_f_x)\n",
    "5. Bias and Variance 계산\n",
    "---\n",
    "np.linear.norm(x) : x라는 행렬의 2 norm을 계산해라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "num_sampling = 1000\n",
    "degree = 1\n",
    "\n",
    "model_list = []\n",
    "exp_f_x = np.zeros(len(x_train))\n",
    "\n",
    "for i in range(num_sampling):\n",
    "    x_train_sample, y_train_sample, y_train_true = get_sample(sample_ratio=0.3,x=x_train,y=y_train)\n",
    "    model = Pipeline([('poly', PolynomialFeatures(degree=degree)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])\n",
    "    model.fit(x_train_sample[:, np.newaxis], y_train_sample)\n",
    "    exp_f_x += model.predict(x_train[:, np.newaxis])\n",
    "    model_list.append(model)\n",
    "\n",
    "\n",
    "exp_f_x/=num_sampling\n",
    "bias = (np.linalg.norm(exp_f_x-y_train))\n",
    "\n",
    "var = 0\n",
    "for j in range(num_sampling):\n",
    "    model = model_list.pop(0)\n",
    "    var = var + np.square(model.predict(x_train[:, np.newaxis])-exp_f_x)\n",
    "var = var/num_sampling\n",
    "variance = np.linalg.norm(var)\n",
    "\n",
    "result_dict[1]['bias'] = bias\n",
    "result_dict[1]['var'] = variance\n",
    "print(\"degree : {} bias : {} variance : {}\".format(degree, bias,variance) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 4 : Make Polynoimal Regression model with degree 5\n",
    "1. Compute the Train , Test MeanSquare Error\n",
    "2. Compute Bias and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(??)\n",
    "\n",
    "model.fit(??)\n",
    "\n",
    "y_predict = model.??  # 예측하는 method를 쓰세요\n",
    "\n",
    "plt.scatter(x_train,y_predict, label='Predicted')\n",
    "plt.plot(x_space,y, label = 'Ground Truth', color='orange')\n",
    "plt.legend()\n",
    "plt.title(\"Ground Truth VS Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = model.??\n",
    "## check Train, Test mse\n",
    "mse_train = mean_squared_error(??,y_predict)\n",
    "mse_test = mean_squared_error(??,??)\n",
    "result_dict[5]['mse_train'] = mse_train\n",
    "result_dict[5]['mse_test'] = mse_test\n",
    "print(\"mse Train : {}, Test : {}\".format(mse_train,mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Bias and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "num_sampling = 1000\n",
    "degree = 5\n",
    "\n",
    "model_list = []\n",
    "exp_f_x = np.zeros(len(x_train))\n",
    "\n",
    "for i in range(num_sampling):\n",
    "    x_train_sample, y_train_sample, y_train_true = ?? # Sample 를 뽑아주는 함수 (Sample ratio = 0.3)\n",
    "    model = ??\n",
    "    model.fit(??)\n",
    "    exp_f_x += model.predict(x_train[:, np.newaxis])\n",
    "    model_list.append(model)\n",
    "\n",
    "\n",
    "exp_f_x/=num_sampling\n",
    "bias = np.linalg.norm(??-??)\n",
    "var = 0\n",
    "for j in range(num_sampling):\n",
    "    model = model_list.pop(0)\n",
    "    var = var + ?? # exp_f_X 과 예측값과의 차이의 제곱!\n",
    "var = var/num_sampling\n",
    "variance = ??\n",
    "result_dict[5]['bias'] = bias\n",
    "result_dict[5]['var'] = variance\n",
    "print(\"degree : {} bias : {} variance : {}\".format(degree, bias,variance) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree 에 따른 Bias/Variance 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(result_dict[1]['bias'],result_dict[1]['var'], label='degree 1', color='black')\n",
    "plt.text(result_dict[1]['bias'] + 0.1,result_dict[1]['var'], \"High Bias, Low Variance\")\n",
    "plt.scatter(result_dict[5]['bias'],result_dict[5]['var'], label='degree 5')\n",
    "plt.text(result_dict[5]['bias'] + 0.1 ,result_dict[5]['var'], \"Low Bias, High Variance\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(3,12)\n",
    "plt.ylim(0,0.7)\n",
    "plt.xlabel(\"Bias\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.title(\"Bias & Variance of degree 1, 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "degree 가 1,2,3,5,10,12 로 증가함에 따라 Bias and Variance 가 어떻게 바뀌는지 확인해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_list = [1,2,3,5,10,12]\n",
    "for d in degree_list:\n",
    "    model_list = []\n",
    "    mse_train = 0\n",
    "    mse_test = 0\n",
    "    exp_f_x = np.zeros(len(x_train))\n",
    "    for i in range(num_sampling):\n",
    "        x_train_sample, y_train_sample, y_train_true = get_sample(sample_ratio=0.40,x=x_train,y=y_train)\n",
    "        model = Pipeline([('poly', PolynomialFeatures(degree=d)),\n",
    "                      ('linear', LinearRegression(fit_intercept=False))])\n",
    "        model.fit(x_train_sample[:, np.newaxis], y_train_sample)\n",
    "        \n",
    "        \n",
    "        # For mse\n",
    "        y_train_sample_predict = model.predict(x_train_sample[:, np.newaxis])\n",
    "        y_test_predict = model.predict(x_test[:,np.newaxis])\n",
    "        mse_train += mean_squared_error(y_train_sample,y_train_sample_predict)\n",
    "        mse_test += mean_squared_error(y_test,y_test_predict)\n",
    "        \n",
    "        exp_f_x += model.predict(x_train[:, np.newaxis])\n",
    "        model_list.append(model)\n",
    "\n",
    "    mse_train /= num_sampling\n",
    "    mse_test /= num_sampling\n",
    "    exp_f_x/=num_sampling\n",
    "\n",
    "    # Bias\n",
    "    bias = (np.linalg.norm(exp_f_x-y_train))\n",
    "    \n",
    "    # Variance\n",
    "    var = 0\n",
    "    for j in range(num_sampling):\n",
    "        model = model_list.pop(0)\n",
    "        var = var + np.square(model.predict(x_train[:, np.newaxis])-exp_f_x)\n",
    "    var = var/num_sampling\n",
    "    variance = np.linalg.norm(var)\n",
    "    print(\"degree : {} bias : {} variance : {}\".format(d, bias,variance) )\n",
    "\n",
    "    result_dict[d] = {'bias':bias,'var':variance,'mse_train':mse_train,'mse_test':mse_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in degree_list:\n",
    "    plt.scatter(result_dict[d]['bias'],result_dict[d]['var'],s=60, label='degree {}'.format(d), \n",
    "                 alpha=0.3)\n",
    "    plt.text(result_dict[d]['bias'],result_dict[d]['var'],'degree {}'.format(d))\n",
    "\n",
    "\n",
    "plt.xlabel(\"Bias\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.title(\"Bias & Variance \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_list = []\n",
    "variance_list = []\n",
    "train_mse_list = []\n",
    "test_mse_list = []\n",
    "for d in degree_list:\n",
    "    bias_list.append(result_dict[d]['bias'])\n",
    "    variance_list.append(result_dict[d]['var'])\n",
    "    train_mse_list.append(result_dict[d]['mse_train'])\n",
    "    test_mse_list.append(result_dict[d]['mse_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degree_list,bias_list, label='Bias')\n",
    "plt.plot(degree_list,variance_list, label='Variance')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Bias and Variance Trade off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degree_list,train_mse_list, label= 'mse Train')\n",
    "plt.plot(degree_list,test_mse_list, label='mse Test')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Train and Test mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree 가 커질수록 항상 좋을까요?\n",
    "그래프를 그려보면 degree가 증가할수록 Train, Test MSE 감소하는 경향을 볼 수 있습니다 <br>\n",
    "과연 Degree가 클수록 항상 좋은걸까요? <br>\n",
    "Degree가 증가하면, Train data를 잘 학습하여셔 Bias는 감소합니다. <br>\n",
    "하지만 Training Data에 노이즈가 강하게 들어가 있으면 어떻게 될까요? <br>\n",
    "노이즈 마저 학습하여 Test MSE가 오히려 더 높아질 수 있습니다 . 아래 실습을 통해 해당 상황을 알아봅시다\n",
    "\n",
    "---\n",
    "먼저 -20. -1 범위에서 추출한 y_noise data에 넣어줍시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "x_noise = [random.uniform(4,6) for i in range(10)]\n",
    "y_noise = [random.uniform(-20,-1) for i in range(10)]\n",
    "plt.scatter(x_train,y_train, label='Train data')\n",
    "plt.scatter(x_noise,y_noise, label='Noise data')\n",
    "plt.legend()\n",
    "plt.title(\"Train Data with noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_noise = np.append(x_train,x_noise)\n",
    "y_train_noise = np.append(y_train,y_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice 5\n",
    "* 위에서 배운 방법들을 이용해서 데이터에 노이즈가 심한 상황에서 Degree가 5, 10 일 때의 다음 값들을 구하세요\n",
    "* Bias , Variance, Train mse , Test mse  \n",
    "* Ground Truth (y_true) 와 Predicted 비교\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case1 : Degree 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ??\n",
    "\n",
    "model.??\n",
    "\n",
    "y_predict = ??\n",
    "\n",
    "plt.scatter(x_train_noise,y_predict, label='Predicted')\n",
    "plt.plot(x_space,y, label = 'Ground Truth', color='orange')\n",
    "plt.legend()\n",
    "plt.title(\"Ground Truth VS Predicted\")\n",
    "\n",
    "y_true = ??\n",
    "y_test_predict = ??\n",
    "y_test_true = ??\n",
    "\n",
    "## check Train, Test mse\n",
    "mse_train = ??\n",
    "mse_test = ??\n",
    "result_dict[5]['mse_train'] = mse_train\n",
    "result_dict[5]['mse_test'] = mse_test\n",
    "print(\"mse Train : {} Test : {}\".format(mse_train,mse_test))\n",
    "    \n",
    "# # Variance, Bias\n",
    "# mse = np.mean((y_predict - y_true) ** 2)\n",
    "# var = np.var(y_predict)\n",
    "# bias = abs(mse - var)\n",
    "# print(\"Variance : {} \\nBias : {}\".format(var,bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "num_sampling = 1000\n",
    "degree = 5\n",
    "\n",
    "model_list = []\n",
    "exp_f_x = np.zeros(len(x_train_noise))\n",
    "\n",
    "for i in range(num_sampling):\n",
    "    x_train_sample, y_train_sample, y_train_true = ?? # Sample 를 뽑아주는 함수\n",
    "    model = ??\n",
    "    model.?? # Model을 train 해주는 method\n",
    "    exp_f_x += ??\n",
    "    model_list.append(model)\n",
    "\n",
    "exp_f_x/=num_sampling\n",
    "bias = ??\n",
    "var = 0\n",
    "for j in range(num_sampling):\n",
    "    model = model_list.pop(0)\n",
    "    var = var + ??\n",
    "var = var/num_sampling\n",
    "variance = ??\n",
    "result_dict[5]['bias'] = bias\n",
    "result_dict[5]['var'] = variance\n",
    "print(\"degree : {} bias : {} variance : {}\".format(degree, bias,variance) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2 : Degree 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('poly', PolynomialFeatures(degree=10)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])\n",
    "\n",
    "model.fit(x_train_noise[:, np.newaxis], y_train_noise)\n",
    "\n",
    "y_predict = model.predict(x_train_noise[:,np.newaxis])\n",
    "\n",
    "plt.scatter(x_train_noise,y_predict, label='Predicted')\n",
    "plt.plot(x_space,y, label = 'Ground Truth', color='orange')\n",
    "plt.legend()\n",
    "plt.title(\"Ground Truth VS Predicted\")\n",
    "\n",
    "y_true = np.sin(x_train_noise)\n",
    "y_test_predict = model.predict(x_test[:,np.newaxis])\n",
    "y_test_true = np.sin(x_test)\n",
    "\n",
    "## check Train, Test mse\n",
    "mse_train = mean_squared_error(y_true,y_predict)\n",
    "mse_test = mean_squared_error(y_test_true,y_test_predict)\n",
    "result_dict[10]['mse_train'] = mse_train\n",
    "result_dict[10]['mse_test'] = mse_test\n",
    "print(\"mse Train : {} Test : {}\".format(mse_train,mse_test))\n",
    "    \n",
    "# # Variance, Bias\n",
    "# mse = np.mean((y_predict - y_true) ** 2)\n",
    "# var = np.var(y_predict)\n",
    "# bias = (mse - var)\n",
    "# print(\"Variance : {} \\nBias : {}\".format(var,bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* More degree of model, gives more flexibility in decision boundarys.\n",
    "* More prone to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "num_sampling = 1000\n",
    "degree = 10\n",
    "\n",
    "model_list = []\n",
    "exp_f_x = np.zeros(len(x_train_noise))\n",
    "\n",
    "for i in range(num_sampling):\n",
    "    x_train_sample, y_train_sample, y_train_true = ?? # Sample 를 뽑아주는 함수\n",
    "    model = ??\n",
    "    model.?? # Model을 train 해주는 method\n",
    "    exp_f_x += ??\n",
    "    model_list.append(model)\n",
    "\n",
    "exp_f_x/=num_sampling\n",
    "bias = ??\n",
    "var = 0\n",
    "for j in range(num_sampling):\n",
    "    model = model_list.pop(0)\n",
    "    var = var + ??\n",
    "var = var/num_sampling\n",
    "variance = ??\n",
    "result_dict[10]['bias'] = bias\n",
    "result_dict[10]['var'] = variance\n",
    "print(\"degree : {} bias : {} variance : {}\".format(degree, bias,variance) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias and Variance Trade off Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_list = []\n",
    "variance_list = []\n",
    "train_mse_list = []\n",
    "test_mse_list = []\n",
    "degree_list = [5,10]\n",
    "for d in degree_list:\n",
    "    bias_list.append(result_dict[d]['bias'])\n",
    "    variance_list.append(result_dict[d]['var'])\n",
    "    train_mse_list.append(result_dict[d]['mse_train'])\n",
    "    test_mse_list.append(result_dict[d]['mse_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degree_list,bias_list, label='Bias')\n",
    "plt.plot(degree_list,variance_list, label='Variance')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Bias and Variance Trade off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degree_list,train_mse_list, label= 'mse Train')\n",
    "plt.plot(degree_list,test_mse_list, label='mse Test')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Train and Test mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsung_bias",
   "language": "python",
   "name": "samsung_bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
